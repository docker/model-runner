name: Release model-runner images for CE
run-name: Release model-runner images for CE, version ${{ inputs.releaseTag }}${{ inputs.pushLatest && ' and latest' || '' }}

on:
  workflow_dispatch:
    inputs:
      pushLatest:
        description: "Tag images produced by this job as latest"
        required: false
        type: boolean
        default: false
      releaseTag:
        description: "Release tag"
        required: false
        type: string
        default: "test"
      llamaServerVersion:
        description: "llama-server version"
        required: false
        type: string
        default: "latest"
      vllmVersion:
        description: "vLLM version"
        required: false
        type: string
        default: "0.12.0"
      sglangVersion:
        description: "SGLang version"
        required: false
        type: string
        default: "0.4.0"
      # This can be removed once we have llama.cpp built for MUSA and CANN.
      buildMusaCann:
        description: "Build MUSA and CANN images"
        required: false
        type: boolean
        default: false

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Summary
        run: |
          echo "## Release Parameters" >> $GITHUB_STEP_SUMMARY
          echo "| Parameter | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Release Tag | \`${{ inputs.releaseTag }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| Push Latest | \`${{ inputs.pushLatest }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| llama-server Version | \`${{ inputs.llamaServerVersion }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| vLLM Version | \`${{ inputs.vllmVersion }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| SGLang Version | \`${{ inputs.sglangVersion }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| Build MUSA/CANN | \`${{ inputs.buildMusaCann }}\` |" >> $GITHUB_STEP_SUMMARY

      - name: Checkout code
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8

      - name: Set up Go
        uses: actions/setup-go@4dc6199c7b1a012772edbd06daecab0f50c9053c
        with:
          go-version: 1.24.3
          cache: true

      - name: Run tests
        run: go test ./...

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8

      - name: Format tags
        id: tags
        shell: bash
        run: |
          format_tags() {
            local variant="$1"
            local suffix="${variant:+-$variant}"  # empty for cpu, "-variant" otherwise
            echo "${variant:-cpu}<<EOF" >> "$GITHUB_OUTPUT"
            echo "docker/model-runner:${{ inputs.releaseTag }}${suffix}" >> "$GITHUB_OUTPUT"
            if [ "${{ inputs.pushLatest }}" == "true" ]; then
              echo "docker/model-runner:latest${suffix}" >> "$GITHUB_OUTPUT"
            fi
            echo 'EOF' >> "$GITHUB_OUTPUT"
          }
          format_tags "" # cpu
          format_tags "cuda"
          format_tags "vllm-cuda"
          format_tags "sglang-cuda"
          format_tags "rocm"
          format_tags "musa"
          format_tags "cann"

      - name: Log in to DockerHub
        uses: docker/login-action@5e57cd118135c172c3672efd75eb46360885c0ef
        with:
          username: "docker"
          password: ${{ secrets.ORG_ACCESS_TOKEN }}

      - name: Set up Buildx
        uses: docker/setup-buildx-action@e468171a9de216ec08956ac3ada2f0791b6bd435
        with:
          version: "lab:latest"
          driver: cloud
          endpoint: "docker/make-product-smarter"
          install: true

      - name: Build CPU image
        uses: docker/build-push-action@263435318d21b8e681c14492fe198d362a7d2c83
        with:
          file: Dockerfile
          target: final-llamacpp
          platforms: linux/amd64, linux/arm64
          build-args: |
            "LLAMA_SERVER_VERSION=${{ inputs.llamaServerVersion }}"
          push: true
          sbom: true
          provenance: mode=max
          tags: ${{ steps.tags.outputs.cpu }}

      - name: Build CUDA image
        uses: docker/build-push-action@263435318d21b8e681c14492fe198d362a7d2c83
        with:
          file: Dockerfile
          target: final-llamacpp
          platforms: linux/amd64, linux/arm64
          build-args: |
            "LLAMA_SERVER_VERSION=${{ inputs.llamaServerVersion }}"
            "LLAMA_SERVER_VARIANT=cuda"
            "BASE_IMAGE=nvidia/cuda:12.9.0-runtime-ubuntu24.04"
          push: true
          sbom: true
          provenance: mode=max
          tags: ${{ steps.tags.outputs.cuda }}

      - name: Build vLLM CUDA image
        uses: docker/build-push-action@263435318d21b8e681c14492fe198d362a7d2c83
        with:
          file: Dockerfile
          target: final-vllm
          platforms: linux/amd64, linux/arm64
          build-args: |
            "LLAMA_SERVER_VERSION=${{ inputs.llamaServerVersion }}"
            "LLAMA_SERVER_VARIANT=cuda"
            "BASE_IMAGE=nvidia/cuda:13.0.2-runtime-ubuntu24.04"
            "VLLM_VERSION=${{ inputs.vllmVersion }}"
            "VLLM_CUDA_VERSION=cu130"
            "VLLM_PYTHON_TAG=cp38-abi3"
          push: true
          sbom: true
          provenance: mode=max
          tags: ${{ steps.tags.outputs.vllm-cuda }}

      - name: Build SGLang CUDA image
        uses: docker/build-push-action@263435318d21b8e681c14492fe198d362a7d2c83
        with:
          file: Dockerfile
          target: final-sglang
          platforms: linux/amd64
          build-args: |
            "LLAMA_SERVER_VERSION=${{ inputs.llamaServerVersion }}"
            "LLAMA_SERVER_VARIANT=cuda"
            "BASE_IMAGE=nvidia/cuda:12.9.0-runtime-ubuntu24.04"
            "SGLANG_VERSION=${{ inputs.sglangVersion }}"
          push: true
          sbom: true
          provenance: mode=max
          tags: ${{ steps.tags.outputs.sglang-cuda }}

      - name: Build ROCm image
        uses: docker/build-push-action@263435318d21b8e681c14492fe198d362a7d2c83
        with:
          file: Dockerfile
          target: final-llamacpp
          platforms: linux/amd64
          build-args: |
            "LLAMA_SERVER_VERSION=${{ inputs.llamaServerVersion }}"
            "LLAMA_SERVER_VARIANT=rocm"
            "BASE_IMAGE=rocm/dev-ubuntu-22.04"
          push: true
          sbom: true
          provenance: mode=max
          tags: ${{ steps.tags.outputs.rocm }}

      - name: Build MUSA image
        if: ${{ inputs.buildMusaCann }}
        uses: docker/build-push-action@263435318d21b8e681c14492fe198d362a7d2c83
        with:
          file: Dockerfile
          target: final-llamacpp
          platforms: linux/amd64
          build-args: |
            "LLAMA_SERVER_VERSION=${{ inputs.llamaServerVersion }}"
            "LLAMA_SERVER_VARIANT=musa"
            "BASE_IMAGE=mthreads/musa:rc4.3.0-runtime-ubuntu22.04-amd64"
          push: true
          sbom: true
          provenance: mode=max
          tags: ${{ steps.tags.outputs.musa }}

      - name: Build CANN image
        if: ${{ inputs.buildMusaCann }}
        uses: docker/build-push-action@263435318d21b8e681c14492fe198d362a7d2c83
        with:
          file: Dockerfile
          target: final-llamacpp
          platforms: linux/arm64, linux/amd64
          build-args: |
            "LLAMA_SERVER_VERSION=${{ inputs.llamaServerVersion }}"
            "LLAMA_SERVER_VARIANT=cann"
            "BASE_IMAGE=ascendai/cann:8.2.rc2-910b-ubuntu22.04-py3.11"
          push: true
          sbom: true
          provenance: mode=max
          tags: ${{ steps.tags.outputs.cann }}
