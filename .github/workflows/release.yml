name: Release model-runner images for CE
run-name: Release model-runner images for CE, version ${{ inputs.releaseTag }}

on:
  workflow_dispatch:
    inputs:
      pushLatest:
        description: 'Tag images produced by this job as latest'
        required: false
        type: boolean
        default: false
      releaseTag:
        description: 'Release tag'
        required: false
        type: string
        default: "test"
      llamaServerVersion:
        description: 'llama-server version'
        required: false
        type: string
        default: "latest"
      vllmVersion:
        description: 'vLLM version'
        required: false
        type: string
        default: "0.11.0"

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@08eba0b27e820071cde6df949e0beb9ba4906955

      - name: Set up Go
        uses: actions/setup-go@d35c59abb061a4a6fb18e82ac0862c26744d6ab5
        with:
          go-version: 1.24.2
          cache: true

      - name: Run tests
        run: go test ./...

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@08eba0b27e820071cde6df949e0beb9ba4906955

      - name: Format tags
        id: tags
        shell: bash
        run: |
          echo "cpu<<EOF" >> "$GITHUB_OUTPUT"
          echo "docker/model-runner:${{ inputs.releaseTag }}" >> "$GITHUB_OUTPUT"
          if [ "${{ inputs.pushLatest }}" == "true" ]; then
            echo "docker/model-runner:latest" >> "$GITHUB_OUTPUT"
          fi
          echo 'EOF' >> "$GITHUB_OUTPUT"
          echo "cuda<<EOF" >> "$GITHUB_OUTPUT"
          echo "docker/model-runner:${{ inputs.releaseTag }}-cuda" >> "$GITHUB_OUTPUT"
          if [ "${{ inputs.pushLatest }}" == "true" ]; then
            echo "docker/model-runner:latest-cuda" >> "$GITHUB_OUTPUT"
          fi
          echo 'EOF' >> "$GITHUB_OUTPUT"
          echo "vllm-cuda<<EOF" >> "$GITHUB_OUTPUT"
          echo "docker/model-runner:${{ inputs.releaseTag }}-vllm-cuda" >> "$GITHUB_OUTPUT"
          if [ "${{ inputs.pushLatest }}" == "true" ]; then
            echo "docker/model-runner:latest-vllm-cuda" >> "$GITHUB_OUTPUT"
          echo "rocm<<EOF" >> "$GITHUB_OUTPUT"
          echo "docker/model-runner:${{ inputs.releaseTag }}-rocm" >> "$GITHUB_OUTPUT"
          if [ "${{ inputs.pushLatest }}" == "true" ]; then
            echo "docker/model-runner:latest-rocm" >> "$GITHUB_OUTPUT"
          fi
          echo 'EOF' >> "$GITHUB_OUTPUT"

      - name: Log in to DockerHub
        uses: docker/login-action@5e57cd118135c172c3672efd75eb46360885c0ef
        with:
          username: "docker"
          password: ${{ secrets.ORG_ACCESS_TOKEN }}

      - name: Set up Buildx
        uses: docker/setup-buildx-action@e468171a9de216ec08956ac3ada2f0791b6bd435
        with:
          version: "lab:latest"
          driver: cloud
          endpoint: "docker/make-product-smarter"
          install: true

      - name: Build CPU image
        uses: docker/build-push-action@ca052bb54ab0790a636c9b5f226502c73d547a25
        with:
          file: Dockerfile
          target: final-llamacpp
          platforms: linux/amd64, linux/arm64
          build-args: |
            "LLAMA_SERVER_VERSION=${{ inputs.llamaServerVersion }}"
          push: true
          sbom: true
          provenance: mode=max
          tags: ${{ steps.tags.outputs.cpu }}

      - name: Build CUDA image
        uses: docker/build-push-action@ca052bb54ab0790a636c9b5f226502c73d547a25
        with:
          file: Dockerfile
          target: final-llamacpp
          platforms: linux/amd64, linux/arm64
          build-args: |
            "LLAMA_SERVER_VERSION=${{ inputs.llamaServerVersion }}"
            "LLAMA_SERVER_VARIANT=cuda"
            "BASE_IMAGE=nvidia/cuda:12.9.0-runtime-ubuntu24.04"
          push: true
          sbom: true
          provenance: mode=max
          tags: ${{ steps.tags.outputs.cuda }}

      - name: Build vLLM CUDA image
        uses: docker/build-push-action@ca052bb54ab0790a636c9b5f226502c73d547a25
        with:
          file: Dockerfile
          target: final-vllm
          platforms: linux/amd64
          build-args: |
            "LLAMA_SERVER_VERSION=${{ inputs.llamaServerVersion }}"
            "LLAMA_SERVER_VARIANT=cuda"
            "BASE_IMAGE=nvidia/cuda:12.9.0-runtime-ubuntu24.04"
            "VLLM_VERSION=${{ inputs.vllmVersion }}"
          push: true
          sbom: true
          provenance: mode=max

      - name: Build ROCm image
        uses: docker/build-push-action@ca052bb54ab0790a636c9b5f226502c73d547a25
        with:
          file: Dockerfile
          platforms: linux/amd64
          build-args: |
            "LLAMA_SERVER_VERSION=${{ inputs.llamaServerVersion }}"
            "LLAMA_SERVER_VARIANT=rocm"
            "BASE_IMAGE=rocm/ubuntu:22.04"
          push: true
          sbom: true
          provenance: mode=max
          tags: ${{ steps.tags.outputs.rocm }}
