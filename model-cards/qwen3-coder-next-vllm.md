# Qwen3-Coder-Next

Qwen3-Coder-Next is an open-weight large language model specifically designed for coding agents and local development environments. With an innovative architecture featuring only 3B activated parameters out of 80B total parameters, it achieves performance comparable to models with 10–20x more active parameters, making it highly cost-effective for agent deployment.

The model excels at advanced agentic capabilities through an elaborate training recipe, demonstrating superior performance in long-horizon reasoning, complex tool usage, and recovery from execution failures. This ensures robust performance in dynamic coding tasks. Its 256k context length, combined with adaptability to various scaffold templates, enables seamless integration with different CLI/IDE platforms such as Claude Code, Qwen Code, Qoder, Kilo, Trae, and Cline.

Qwen3-Coder-Next features a unique hybrid architecture combining Gated DeltaNet for efficient linear attention, Gated Attention for long-range dependencies, and a Mixture of Experts approach that activates 10 out of 512 experts per forward pass, significantly reducing computational requirements while maintaining high performance.

---

## Characteristics

| Attribute | Value |
|---|---|
| **Provider** | Qwen (Alibaba Cloud) |
| **Architecture** | Qwen3Next (Hybrid: Gated DeltaNet, Gated Attention, MoE) |
| **Parameters** | 80B total, 3B activated |
| **Context Length** | 262,144 tokens (256k) |
| **Languages** | Multilingual (coding-focused) |
| **Input modalities** | Text |
| **Output modalities** | Text |
| **License** | Apache 2.0 |

## Using this model with Docker Model Runner

```bash
docker model run qwen3-coder-next-vllm
```

For more information, check out the [Docker Model Runner docs](https://docs.docker.com/desktop/features/model-runner/).

## Benchmarks

According to the Qwen team's evaluation, Qwen3-Coder-Next achieves:

- **Superior performance** compared to models with 10-20x more active parameters
- **Strong results** on SWE-bench Pro benchmark for real-world software engineering tasks
- **Efficient inference** with only 3B activated parameters per forward pass

Detailed benchmark comparisons are available in the [official blog post](https://qwen.ai/blog?id=qwen3-coder-next) and technical report.

## Architecture Details

Qwen3-Coder-Next features a unique 48-layer hybrid architecture:
- **Hybrid Layout**: 12 × (3 × (Gated DeltaNet → MoE) → 1 × (Gated Attention → MoE))
- **Gated Attention**: 16 Q heads, 2 KV heads, 256-dimensional heads
- **Gated DeltaNet**: 32 V heads, 16 QK heads, 128-dimensional heads
- **Mixture of Experts**: 512 total experts, 10 activated per token, plus 1 shared expert
- **Hidden Dimension**: 2048
- **Rotary Position Embedding**: 64 dimensions

## Considerations

- Requires significant GPU memory (multi-GPU setup recommended for full 256k context)
- Tensor parallelism across 2+ GPUs is recommended for optimal performance
- Default 256k context length may require reduction to 32k tokens on memory-constrained systems
- Designed specifically for coding and agentic tasks; may not be optimal for general-purpose use
- Does not support thinking mode (no `<think></think>` blocks)
- Recommended sampling parameters: `temperature=1.0`, `top_p=0.95`, `top_k=40`
- Compatible with vLLM >= 0.15.0 and SGLang >= 0.5.8
- Supports tool calling and function execution for agentic workflows

## Links

- [Hugging Face Model Repository](https://huggingface.co/Qwen/Qwen3-Coder-Next)
- [Official Blog Post](https://qwen.ai/blog?id=qwen3-coder-next)
- [GitHub Repository](https://github.com/QwenLM/Qwen3-Coder)
- [Technical Report (PDF)](https://github.com/QwenLM/Qwen3-Coder/blob/main/qwen3_coder_next_tech_report.pdf)
- [Documentation](https://qwen.readthedocs.io/en/latest/)

### Generated by
This model card was automatically generated using [cagent-action](https://github.com/docker/cagent-action) with the [Docker Model Runner's model-card-generator](https://github.com/docker/model-runner).
